# ax2.set_yticks(np.arange(0, 1+0.2, 0.2))
ax1.set_ylim([0,np.ceil(np.nanmax(distr_prob['distr'].values)*10)/10])
ax2.set_ylim([0,1])
plt.xticks(ind, distr_prob.index)
plt.title(title_string, loc='left')
ax1.legend((p1[0], p2[0]), list(distr_prob.columns.levels[1]), loc='upper left')
ax2.legend((p3[0], p4[0]), list(distr_prob.columns.levels[1]), loc='upper right')
# show plot
plt.show()
# return of pic
rt_pic[sn] = fig
# return distr_dat ------
if return_distr_dat:
rt_dat[sn] = distr_prob[['N','badprob']].reset_index()
# return rt
rt['psi'] = pd.concat(rt_psi).reset_index().rename(columns={'level_0':'variable'})[['variable', 'PSI']]
rt['pic'] = rt_pic
if return_distr_dat: rt['dat'] = rt_dat
return rt
import pandas as pd
import re
# coefficients in scorecard
def ab(points0=600, odds0=1/60, pdo=50):
# sigmoid function
# library(ggplot2)
# ggplot(data.frame(x = c(-5, 5)), aes(x)) + stat_function(fun = function(x) 1/(1+exp(-x)))
# log_odds function
# ggplot(data.frame(x = c(0, 1)), aes(x)) + stat_function(fun = function(x) log(x/(1-x)))
# logistic function
# p(y=1) = 1/(1+exp(-z)),
# z = beta0+beta1*x1+...+betar*xr = beta*x
##==> z = log(p/(1-p)),
# odds = p/(1-p) # bad/good <==>
# p = odds/1+odds
##==> z = log(odds)
##==> score = a - b*log(odds)
# two hypothesis
# points0 = a - b*log(odds0)
# points0 - PDO = a - b*log(2*odds0)
b = pdo/np.log(2)
a = points0 + b*np.log(odds0) #log(odds0/(1+odds0))
return {'a':a, 'b':b}
#' Creating a Scorecard
#'
#' \code{scorecard} creates a scorecard based on the results from \code{woebin} and \code{glm}.
#'
#' @param bins Binning information generated from \code{woebin} function.
#' @param model A glm model object.
#' @param points0 Target points, default 600.
#' @param odds0 Target odds, default 1/19. Odds = p/(1-p).
#' @param pdo Points to Double the Odds, default 50.
#' @param basepoints_eq0 Logical, default is FALSE. If it is TRUE, the basepoints will equally distribute to each variable.
#' @return scorecard
#'
#' @seealso \code{\link{scorecard_ply}}
#'
#' @examples
#' \dontrun{
#' # load germancredit data
#' data("germancredit")
#'
#' # filter variable via missing rate, iv, identical value rate
#' dt_sel = var_filter(germancredit, "creditability")
#'
#' # woe binning ------
#' bins = woebin(dt_sel, "creditability")
#' dt_woe = woebin_ply(dt_sel, bins)
#'
#' # glm ------
#' m = glm(creditability ~ ., family = binomial(), data = dt_woe)
#' # summary(m)
#'
#' # Select a formula-based model by AIC
#' m_step = step(m, direction="both", trace=FALSE)
#' m = eval(m_step$call)
#' # summary(m)
#'
#' # predicted proability
#' # dt_pred = predict(m, type='response', dt_woe)
#'
#' # performace
#' # ks & roc plot
#' # perf_eva(dt_woe$creditability, dt_pred)
#'
#' # scorecard
#' # Example I # creat a scorecard
#' card = scorecard(bins, m)
#'
#' # credit score
#' # Example I # only total score
#' score1 = scorecard_ply(dt, card)
#'
#' # Example II # credit score for both total and each variable
#' score2 = scorecard_ply(dt, card, only_total_score = F)
#' }
#' @import data.table
#' @export
#'
def scorecard(bins, model, xcolumns, points0=600, odds0=1/19, pdo=50, basepoints_eq0=False):
# coefficients
aabb = ab(points0, odds0, pdo)
a, b = aabb.values()
# odds = pred/(1-pred); score = a - b*log(odds)
# bins # if (is.list(bins)) rbindlist(bins)
if isinstance(bins, dict):
bins = pd.concat(bins, ignore_index=True)
xs = [re.sub('_woe$', '', i) for i in xcolumns]
# coefficients
coef_df = pd.Series(model.coef_[0], index=np.array(xs))\
.loc[lambda x: x != 0]#.reset_index(drop=True)
# scorecard
len_x = len(coef_df)
basepoints = a - b*model.intercept_[0]
card = {}
if basepoints_eq0:
card['basepoints'] = pd.DataFrame({'variable':"basepoints", 'bin':np.nan, 'points':0}, index=np.arange(1))
for i in coef_df.index:
card[i] = bins.loc[bins['variable']==i,['variable', 'bin', 'woe']]\
.assign(points = lambda x: round(-b*x['woe']*coef_df[i] + basepoints/len_x))\
[["variable", "bin", "points"]]
else:
card['basepoints'] = pd.DataFrame({'variable':"basepoints", 'bin':np.nan, 'points':round(basepoints)}, index=np.arange(1))
for i in coef_df.index:
card[i] = bins.loc[bins['variable']==i,['variable', 'bin', 'woe']]\
.assign(points = lambda x: round(-b*x['woe']*coef_df[i]))\
[["variable", "bin", "points"]]
return card
#' Score Transformation
#'
#' \code{scorecard_ply} calculates credit score using the results from \code{scorecard}.
#'
#' @param dt Original data
#' @param card Scorecard generated from \code{scorecard}.
#' @param only_total_score  Logical, default is TRUE. If it is TRUE, then the output includes only total credit score; Otherwise, if it is FALSE, the output includes both total and each variable's credit score.
#' @param print_step A non-negative integer. Default is 1. If print_step>0, print variable names by each print_step-th iteration. If print_step=0, no message is print.
#' @return Credit score
#'
#' @seealso \code{\link{scorecard}}
#'
#' @examples
#' \dontrun{
#' # load germancredit data
#' data("germancredit")
#'
#' # filter variable via missing rate, iv, identical value rate
#' dt_sel = var_filter(germancredit, "creditability")
#'
#' # woe binning ------
#' bins = woebin(dt_sel, "creditability")
#' dt_woe = woebin_ply(dt_sel, bins)
#'
#' # glm ------
#' m = glm(creditability ~ ., family = binomial(), data = dt_woe)
#' # summary(m)
#'
#' # Select a formula-based model by AIC
#' m_step = step(m, direction="both", trace=FALSE)
#' m = eval(m_step$call)
#' # summary(m)
#'
#' # predicted proability
#' # dt_pred = predict(m, type='response', dt_woe)
#'
#' # performace
#' # ks & roc plot
#' # perf_eva(dt_woe$creditability, dt_pred)
#'
#' # scorecard
#' # Example I # creat a scorecard
#' card = scorecard(bins, m)
#'
#' # credit score
#' # Example I # only total score
#' score1 = scorecard_ply(dt, card)
#'
#' # Example II # credit score for both total and each variable
#' score2 = scorecard_ply(dt, card, only_total_score = F)
#' }
#' @import data.table
#' @export
#'
def scorecard_ply(dt, card, only_total_score=True, print_step=0):
# remove date/time col
dt = rm_datetime_col(dt)
# replace "" by NA
dt = rep_blank_na(dt)
# print_step
print_step = check_print_step(print_step)
# card # if (is.list(card)) rbindlist(card)
if isinstance(card, dict):
card_df = pd.concat(card, ignore_index=True)
# x variables
xs = card_df.loc[card_df.variable != 'basepoints', 'variable'].unique()
# length of x variables
xs_len = len(xs)
# initial datasets
dat = dt.loc[:,list(set(dt.columns)-set(xs))]
# loop on x variables
for i in np.arange(xs_len):
x_i = xs[i]
# print xs
if print_step>0 and bool((i+1)%print_step):
print(('{:'+str(len(str(xs_len)))+'.0f}/{} {}').format(i, xs_len, x_i))
cardx = card_df.loc[card_df['variable']==x_i]
dtx = dt[[x_i]]
# score transformation
dtx_points = woepoints_ply1(dtx, cardx, x_i, woe_points="points")
dat = pd.concat([dat, dtx_points], axis=1)
# set basepoints
card_basepoints = list(card_df.loc[card_df['variable']=='basepoints','points'])[0] if 'basepoints' in card_df['variable'].unique() else 0
# total score
dat_score = dat[xs+'_points']
dat_score.loc[:,'score'] = card_basepoints + dat_score.sum(axis=1)
# return
if only_total_score: dat_score = dat_score[['score']]
return dat_score
import pandas as pd
import numpy as np
import warnings
#' Split a dataset
#'
#' @param dt A data frame.
#' @param y Name of y variable, default is NULL. The input data will split based on the predictor y, if it is provide.
#' @param ratio A numeric value, default is 0.7. It indicates the ratio of total rows contained in one split, must less than 1.
#' @param seed A random seed, default is 186.
#'
#' @examples
#' # Load German credit data
#' data(germancredit)
#'
#' dt_list = split_df(germancredit, y="creditability")
#' train = dt_list$train
#' test = dt_list$test
#'
#' @import data.table
#' @export
def split_df(dt, y=None, ratio=0.7, seed=186):
# remove date/time col
dt = rm_datetime_col(dt)
# replace "" by NA
dt = rep_blank_na(dt)
# set ratio range
if not isinstance(ratio, (float, int)) or ratio > 1 or ratio <= 0:
warnings.warn("Incorrect inputs; ratio must be a numeric that length equal to 1 and less than 1. It was set to 0.7.")
ratio = 0.7
# split into train and test
if y is None:
train = dt.sample(frac=ratio, random_state=seed).sort_index()
test = dt.iloc[list(set(dt.index.tolist()).difference(set(train.index.tolist())))].sort_index()
else:
train = dt.groupby(y)\
.apply(lambda x: x.sample(frac=ratio, random_state=seed))\
.reset_index(level=y, drop=True)\
.sort_index()
test = dt.iloc[list(set(dt.index.tolist()).difference(set(train.index.tolist())))].sort_index()
return {'train': train, 'test': test}
import pandas as pd
import warnings
#' Variable Filter
#'
#' This function filter variables base on specified conditions, such as information value, missing rate, identical value rate.
#'
#' @param dt A data frame with both x (predictor/feature) and y (response/label) variables.
#' @param y Name of y variable.
#' @param x Name of x variables. Default is NULL. If x is NULL, then all variables except y are counted as x variables.
#' @param iv_limit The information value of kept variables should >= iv_limit. The default is 0.02.
#' @param missing_limit The missing rate of kept variables should <= missing_limit. The default is 0.95.
#' @param identical_limit The identical value rate (excluding NAs) of kept variables should <= identical_limit. The default is 0.95.
#' @param var_rm Name of force removed variables, default is NULL.
#' @param var_kp Name of force kept variables, default is NULL.
#' @param return_rm_reason Logical, default is FALSE.
#' @param positive Value of positive class, default is "bad|1".
#' @return A data.table with y and selected x variables and a data.table with the reason of removed x variable if return_rm_reason == TRUE.
#'
#' @examples
#' # Load German credit data
#' data(germancredit)
#'
#' # variable filter
#' dt_sel = var_filter(germancredit, y = "creditability")
#'
#'
#' @import data.table
#' @export
#'
def var_filter(dt, y, x=None, iv_limit=0.02, missing_limit=0.95, identical_limit=0.95, var_rm=None, var_kp=None, return_rm_reason=False, positive='bad|1'):
# remove date/time col
dt = rm_datetime_col(dt)
# replace "" by NA
dt = rep_blank_na(dt)
# check y
dt = check_y(dt, y, positive)
# x variable names
x = x_variable(dt,y,x)
# force removed variables
if var_rm is not None: x = list(set(x).difference(set(var_rm)))
# check force kept variables
if var_kp is not None:
var_kp2 = set(var_kp) & set(x)
len_diff_var_kp = len(var_kp) - len(var_kp2)
if len_diff_var_kp > 0:
warnings.warn("Incorrect inputs; there are {} var_kp variables are not exist in input data, which are removed from var_kp. \n {}".format(len_diff, list(set(var_kp)-set(var_kp2))) )
var_kp = var_kp2 if len(var_kp2)>0 else None
# -iv
iv_list = iv(dt, y, x, order=False)
# -na percentage
nan_rate = lambda a: a[a.isnull()].size/a.size
na_perc = dt[x].apply(nan_rate).reset_index(name='missingrate').rename(columns={'index':'variable'})
# -identical percentage
idt_rate = lambda a: a.value_counts().max() / a.size
identical_perc = dt[x].apply(idt_rate).reset_index(name='identicalrate').rename(columns={'index':'variable'})
# dataframe iv na idt
dt_var_selector = iv_list.merge(na_perc,on='variable').merge(identical_perc,on='variable')
# remove na_perc>95 | ele_perc>0.95 | iv<0.02
# variable datatable selected
dt_var_sel = dt_var_selector.query('(info_value >= {}) & (missingrate <= {}) & (identicalrate <= {})'.format(iv_limit,missing_limit,identical_limit))
# add kept variable
x_selected = dt_var_sel.variable.tolist()
if var_kp is not None: x_selected = np.unique(x_selected+var_kp).tolist()
# data kept
dt_kp = dt[x_selected+[y]]
# return remove reason
if return_rm_reason:
dt_var_rm = dt_var_selector.query('(info_value < {}) | (missingrate > {}) | (identicalrate > {})'.format(iv_limit,missing_limit,identical_limit)) \
.assign(
info_value = lambda x: ['info_value<{}'.format(iv_limit) if i else np.nan for i in (x.info_value < iv_limit)],
missingrate = lambda x: ['missingrate>{}'.format(missing_limit) if i else np.nan for i in (x.missingrate > missing_limit)],
identicalrate = lambda x: ['identicalrate>{}'.format(identical_limit) if i else np.nan for i in (x.identicalrate > identical_limit)]
)
dt_rm_reason = pd.melt(dt_var_rm, id_vars=['variable'], var_name='iv_mr_ir').dropna()\
.groupby('variable').apply(lambda x: ', '.join(x.value)).reset_index(name='rm_reason')
if var_rm is not None:
dt_rm_reason = pd.concat([
dt_rm_reason,
pd.DataFrame({'variable':var_rm,'rm_reason':"force remove"}, columns=['variable', 'rm_reason'])
])
if var_kp is not None:
dt_rm_reason = dt_rm_reason.query('variable not in {}'.format(var_kp))
return {'dt': dt_kp, 'rm':dt_rm_reason}
else:
return dt_kp
#' German Credit Data
#'
#' Credit data that classifies debtors described by
#' a set of attributes as good or bad credit risks.
#' See source link below for detailed information.
#'
#' @docType data
#' @keywords data
#' @name germancredit
#' @usage data(germancredit)
#' @format A data frame with 21 variables
#' (numeric and factors) and 1000 observations.
#' @source \url{https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)}
#' @examples
#' # Load German credit data and create subset
#' data(germancredit)
#' df = germancredit[, c('creditability', 'credit.amount', 'duration.in.month',
#'                   'savings.account.and.bonds', 'purpose')]
#' # Display structure of the subset (data frame)
#' str(df)
import pandas as pd
from pandas.api.types import CategoricalDtype
def germancredit():
dat = pd.read_csv("./data/germancredit.csv")
# categorical levels
cate_levels = {
"status.of.existing.checking.account": ['... < 0 DM', '0 <= ... < 200 DM', '... >= 200 DM / salary assignments for at least 1 year', 'no checking account'],
"credit.history": ["no credits taken/ all credits paid back duly", "all credits at this bank paid back duly", "existing credits paid back duly till now", "delay in paying off in the past", "critical account/ other credits existing (not at this bank)"],
"savings.account.and.bonds": ["... < 100 DM", "100 <= ... < 500 DM", "500 <= ... < 1000 DM", "... >= 1000 DM", "unknown/ no savings account"],
"present.employment.since": ["unemployed", "... < 1 year", "1 <= ... < 4 years", "4 <= ... < 7 years", "... >= 7 years"],
"personal.status.and.sex": ["male : divorced/separated", "female : divorced/separated/married", "male : single", "male : married/widowed", "female : single"],
"other.debtors.or.guarantors": ["none", "co-applicant", "guarantor"],
"property": ["real estate",  "building society savings agreement/ life insurance",  "car or other, not in attribute Savings account/bonds",  "unknown / no property"],
"other.installment.plans": ["bank", "stores", "none"],
"housing": ["rent", "own", "for free"],
"job": ["unemployed/ unskilled - non-resident", "unskilled - resident", "skilled employee / official", "management/ self-employed/ highly qualified employee/ officer"],
"telephone": ["none", "yes, registered under the customers name"],
"foreign.worker": ["yes", "no"]}
# func of cate
def cate_type(levels):
return CategoricalDtype(categories=levels, ordered=True)
# to cate
for i in cate_levels.keys():
dat[i] = dat[i].astype(cate_type(cate_levels[i]))
# return
return dat
# the dataset is modified from woebinning package
'''
# Traditional Credit Scoring Using Logistic Regression
import scorecardpy
# data prepare ------
# load germancredit data
dat = germancredit()
# filter variable via missing rate, iv, identical value rate
dt_s = var_filter(dat, y="creditability")
# breaking dt into train and test
train, test = split_df(dt_s, 'creditability').values()
# woe binning ------
bins = woebin(dt_s, y="creditability")
# woebin_plot(bins)
# binning adjustment
# # adjust breaks interactively
# breaks_adj = woebin_adj(dt_s, "creditability", bins)
# # or specify breaks manually
breaks_adj = {
'age.in.years': [26, 35, 40],
'other.debtors.or.guarantors': ["none", "co-applicant%,%guarantor"]
}
bins_adj = woebin(dt_s, y="creditability", breaks_list=breaks_adj)
# converting train and test into woe values
train_woe = woebin_ply(train, bins_adj)
test_woe = woebin_ply(test, bins_adj)
y_train = train_woe[['creditability']].loc[:,'creditability']
X_train = train_woe.loc[:,train_woe.columns != 'creditability']
y_test = test_woe[['creditability']].loc[:,'creditability']
X_test = test_woe.loc[:,train_woe.columns != 'creditability']
# logistic regression ------
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(penalty='l1', C=0.9, solver='saga')
lr.fit(X_train, y_train)
# lr.coef_
# lr.intercept_
# predicted proability
pred_train = lr.predict_proba(X_train)[:,1]
pred_test = lr.predict_proba(X_test)[:,1]
# performance ks & roc ------
perf_train = perf_eva(y_train, pred_train, title = "train")
perf_test = perf_eva(y_test, pred_test, title = "test")
# score ------
card = scorecard(bins_adj, lr, X_train.columns)
# credit score
train_score = scorecard_ply(train, card, print_step=0)
test_score = scorecard_ply(test, card, print_step=0)
# psi
psirt = perf_psi(
score = {'train':train_score, 'test':test_score},
label = {'train':y_train, 'test':y_test},
x_limits = [250, 750],
x_tick_break = 50
)
# datasets
dat1 = germancredit()
dat1 = check_y(dat1, 'creditability', 'bad|1')
dat2 = pd.DataFrame({'creditability':[0,1]}).sample(50, replace=True)
# dat2 = pd.DataFrame({'creditability':np.random.choice([0,1], 50)})
dat = pd.concat([dat2, dat1], ignore_index=True)
###### dtm ######
# y
y = dat['creditability']
# x
# numerical data
xvar =  'age.in.years' #'number.of.existing.credits.at.this.bank' # "credit.amount" # "foreign.worker
x= dat[xvar]
spl_val = [2600, 9960, "6850%,%missing"]
breaks = [2000, 4000, 6000]
breaks = ['26%,%missing', 28, 35, 37]
# categorical data
xvar= 'housing' # "job" # "credit.amount"; #
x= pd.Categorical(dat[xvar], categories=['rent', 'own','for free'])
breaks = ["own", "for free%,%rent%,%missing"]
breaks = ["own", "for free%,%rent"]
dtm = pd.DataFrame({'y':y, 'variable':xvar, 'value':x})
# dtm.value = None
'''
dat = germancredit()
# filter variable via missing rate, iv, identical value rate
dt_s = var_filter(dat, y="creditability")
# breaking dt into train and test
train, test = split_df(dt_s, 'creditability').values()
# woe binning ------
bins = woebin(dt_s, y="creditability")
# woebin_plot(bins)
# binning adjustment
# # adjust breaks interactively
# breaks_adj = woebin_adj(dt_s, "creditability", bins)
# # or specify breaks manually
breaks_adj = {
'age.in.years': [26, 35, 40],
'other.debtors.or.guarantors': ["none", "co-applicant%,%guarantor"]
}
bins_adj = woebin(dt_s, y="creditability", breaks_list=breaks_adj)
# converting train and test into woe values
train_woe = woebin_ply(train, bins_adj)
test_woe = woebin_ply(test, bins_adj)
y_train = train_woe[['creditability']].loc[:,'creditability']
X_train = train_woe.loc[:,train_woe.columns != 'creditability']
y_test = test_woe[['creditability']].loc[:,'creditability']
X_test = test_woe.loc[:,train_woe.columns != 'creditability']
# logistic regression ------
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(penalty='l1', C=0.9, solver='saga')
lr.fit(X_train, y_train)
# lr.coef_
# lr.intercept_
# predicted proability
pred_train = lr.predict_proba(X_train)[:,1]
pred_test = lr.predict_proba(X_test)[:,1]
# performance ks & roc ------
perf_train = perf_eva(y_train, pred_train, title = "train")
perf_test = perf_eva(y_test, pred_test, title = "test")
card = scorecard(bins_adj, lr, X_train.columns)
train_score = scorecard_ply(train, card, print_step=0)
test_score = scorecard_ply(test, card, print_step=0)
from scorecardpy import *
import scorecardpy
import scorecardpy
reticulate::repl_python()
import scorecardpy as sc
reticulate::repl_python()
dat = sc.germancredit()
reticulate::repl_python()
